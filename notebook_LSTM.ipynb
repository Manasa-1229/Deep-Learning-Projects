{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c79aee2",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\">Big Data Analytics</h3>\n",
    "<h3 style=\"text-align: center;\">LSTM Storyteller</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32654b",
   "metadata": {},
   "source": [
    "Once upon a time, there was a storyteller who wanted to create new fairy tales. The storyteller knew that to do this, they needed a powerful tool that could learn from existing stories and generate new ones. So, they decided to build an LSTM-based story generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42b4285",
   "metadata": {
    "id": "c310a0c5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d953f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the text corpus\n",
    "with open('LSTM/cleaned_merged_fairy_tales_without_eos.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus_text = file.read().lower()  # Read and convert to lowercase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b5092",
   "metadata": {},
   "source": [
    "The storyteller began by loading and preprocessing a corpus of fairy tales. They then tokenized the text, removed special characters and numbers, and removed stop words. Next, the storyteller used the Tokenizer from Keras to convert the text into sequences and generate a categorical variable. They also defined a function to generate sequences of tokens to train the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeabd9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8993\n",
      "<class 'str'>\n",
      "1248189\n"
     ]
    }
   ],
   "source": [
    "corpus= corpus_text.split('.')\n",
    "part = int(len(corpus)*.05)\n",
    "part_corpus=corpus[:part]\n",
    "print(len(part_corpus))\n",
    "corpus = \". \".join(part_corpus)\n",
    "print(type(corpus))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2508f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manug\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manug\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Convert text to lowercase\n",
    "text = corpus.lower()\n",
    "# Remove special characters and numbers\n",
    "text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#tokens = [word for word in tokens if not word in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4280bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized words: 9649\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the text\n",
    "token_type = 'word'\n",
    "if token_type == 'word':\n",
    "    tokenizer = Tokenizer(char_level = False, filters = '')\n",
    "else:\n",
    "    tokenizer = Tokenizer(char_level = True, filters = '', lower = False)\n",
    "\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# printing interesting quntities:\n",
    "print(f\"Number of tokenized words: {total_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d990af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 237293 \n",
      "\n",
      "Inout shape: (237293, 20)\n",
      "Output shape: (237293, 9649)\n"
     ]
    }
   ],
   "source": [
    "def generate_sequences(token_list, step):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(0, len(token_list) - seq_length, step):\n",
    "        X.append(token_list[i: i + seq_length])\n",
    "        y.append(token_list[i + seq_length])\n",
    "    \n",
    "    # one-hot encoding, creating a categorical variable:\n",
    "    y = np_utils.to_categorical(y, num_classes = total_words)\n",
    "    \n",
    "    num_seq = len(X)\n",
    "    print('Number of sequences:', num_seq, \"\\n\")\n",
    "    \n",
    "    return X, y, num_seq\n",
    "\n",
    "step = 1\n",
    "seq_length = 20\n",
    "\n",
    "X, y, num_seq = generate_sequences(token_list, step)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# printing output:\n",
    "print(f\"Inout shape: {X.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7346fd",
   "metadata": {},
   "source": [
    "After defining the model architecture with an input layer, an embedding layer, an LSTM layer, and a dense layer, the storyteller compiled the model with the RMSprop optimizer and trained it using the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bbdf922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         964900    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               365568    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 9649)              2479793   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,810,261\n",
      "Trainable params: 3,810,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_units = 256\n",
    "embedding_size = 100\n",
    "\n",
    "teInput_datat_in = Input(shape = (None,))\n",
    "embedding = Embedding(total_words, embedding_size)\n",
    "Input_data = embedding(text_in)\n",
    "Input_data = LSTM(n_units)(Input_data)\n",
    "teInput_datat_out = Dense(total_words, activation = 'softmaInput_data')(Input_data)\n",
    "\n",
    "model = Model(text_in, text_out)\n",
    "learning_rate = 0.001\n",
    "opti = RMSprop(learning_rate = learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opti)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027c612",
   "metadata": {},
   "source": [
    "Once the model was trained, the storyteller saved it to a file using the pickle library. They also defined a function called `story_text` that takes a seed text, the number of words to generate, the trained model, the maximum sequence length, and a temperature value as input. The function then generates new stories using the trained LSTM model and the seed text provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2dcf3352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7416/7416 [==============================] - 387s 52ms/step - loss: 2.9022\n",
      "Epoch 2/10\n",
      "7416/7416 [==============================] - 395s 53ms/step - loss: 2.8848\n",
      "Epoch 3/10\n",
      "7416/7416 [==============================] - 432s 58ms/step - loss: 2.8817\n",
      "Epoch 4/10\n",
      "7416/7416 [==============================] - 442s 60ms/step - loss: 2.8798\n",
      "Epoch 5/10\n",
      "7416/7416 [==============================] - 419s 56ms/step - loss: 2.8705\n",
      "Epoch 6/10\n",
      "7416/7416 [==============================] - 438s 59ms/step - loss: 2.8624\n",
      "Epoch 7/10\n",
      "7416/7416 [==============================] - 422s 57ms/step - loss: 2.8589\n",
      "Epoch 8/10\n",
      "7416/7416 [==============================] - 388s 52ms/step - loss: 2.8505\n",
      "Epoch 9/10\n",
      "7416/7416 [==============================] - 372s 50ms/step - loss: 2.8548\n",
      "Epoch 10/10\n",
      "7416/7416 [==============================] - 401s 54ms/step - loss: 2.8401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21784dc61a0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "# this will take a while ...\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36a68cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('storteller_lstm_3.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fda5c2",
   "metadata": {},
   "source": [
    "The storyteller also created a chatbot function called `story_chat` that takes user input and generates a response using the `story_text` function. They integrated the chatbot function with a speech-to-text library and a text-to-speech library to allow users to speak their input and hear the bot's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "57577223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_text(seed_text, next_words, model, max_sequence_len, temp):\n",
    "    output_text = seed_text\n",
    "    #seed_text = start_story + seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = token_list[-max_sequence_len:]\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len))\n",
    "        \n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        pred = pred_temp(probs, temperature = temp)\n",
    "        \n",
    "        if pred == 0:\n",
    "            output_word = ''\n",
    "        else:\n",
    "            output_word = tokenizer.index_word[pred]\n",
    "            \n",
    "        if token_type == 'word':\n",
    "            output_text += output_word + \" \"\n",
    "            seed_text += output_word + \" \"\n",
    "        else:\n",
    "            output_text += output_word + \" \" \n",
    "            seed_text += output_word + \" \"\n",
    "            \n",
    "    return output_text\n",
    "\n",
    "def pred_temp(preds, temperature=0.9):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e3f93ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp 0.1\n",
      "Once upon a time there was a little girl in a village waiting for a prince to start her exciting life in this wonderful world i will do not forget to you herself in another now she took hold of the fountain as beautiful do you shall the charming charming princess asked beauty if she could not help because he was going to ask for the terrible that he could fetch you and see air in the air because he would not ask his good the boy to let him go on the road open but he there stood his he who asked for the fast in he had a good house for he was very good and beautiful his long golden apple are beautiful \n",
      "Temp 0.5\n",
      "Once upon a time there was a little girl in a village waiting for a prince to start her exciting life in this wonderful world i must do that she from cried out to go her bring another terrible most eat me to morrow do off i do you take the way said asked the mice do not stay for answered pinocchio then the king s wife streets on the account of the ship would take our two the shall be cats in the palace and saw there she no forced me to the stable and the them shall not tell how he said to you do me better than the he asked for you the lie in the me have never forget and i \n",
      "Temp 0.9\n",
      "Once upon a time there was a little girl in a village waiting for a prince to start her exciting life in this wonderful world i can asked you take a very beautiful said she my name clothes for my name can live and without your once she will said the king well do you forget my foot and a very bad nose it was almost because you madam made me as in it you make you such a kind of tell you you shall find out what you have done you lands said the prince you must have been able to do as you say a marry you and replied the old prince do you know in it because he is very matter in \n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Once upon a time there was a little girl in a village waiting for a prince to start her exciting life in this wonderful world \"\n",
    "gen_words = 100\n",
    "seq_length= 20\n",
    "print('Temp 0.1')\n",
    "print (story_text(seed_text, gen_words, model, seq_length, temp = 0.1))\n",
    "print('Temp 0.5')\n",
    "print (story_text(seed_text, gen_words, model, seq_length, temp = 0.5))\n",
    "print('Temp 0.9')\n",
    "print (story_text(seed_text, gen_words, model, seq_length, temp = 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a2312422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the chatbot\n",
    "def story_chat(user_input,gen_words):\n",
    "    try:\n",
    "\n",
    "        while True:\n",
    "            #user_input = input(\"You: \")\n",
    "            if user_input.lower() == 'bye':\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            print('hello')\n",
    "            response = story_text(user_input, gen_words, model, seq_length, temp = .9)\n",
    "            print(response)\n",
    "            return response\n",
    "    except:\n",
    "        return \"I can't understand, please say something else\"\n",
    "def ask_bot_story():\n",
    "    from speechToText import speech_to_text\n",
    "    from text_to_speech import speak\n",
    "    speak('Say minimum 10 words to begin a story')    \n",
    "    user_input = speech_to_text()\n",
    "    #speak('How many words in the story?')\n",
    "    #numbers = speech_to_text()\n",
    "    gen_words = 250\n",
    "    #print(gen_words)\n",
    "    response = story_chat(user_input,gen_words)\n",
    "    speak(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2eeb34",
   "metadata": {},
   "source": [
    "In summary, the LSTM-based story generator built by the storyteller allows for the generation of new fairy tales by learning from existing stories. The chatbot function also allows users to interact with the model and generate stories using voice commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0643cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "You said: a girl found a haunted place in a secluded area inside a forest and then she found her family\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manug\\AppData\\Local\\Temp\\ipykernel_76336\\1223746333.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a girl found a haunted place in a secluded area inside a forest and then she found her familythat against against by against begged these his begged himself against against by who who your which gave them against against against we head six each which show himself your against begged against your aladdin against begged himself against against by who your whom father which them against few each that your begged himself by against against against by who against against these that ah majesty majesty our against against where against begged himself your begged against by against against by or each go against each begged himself against against against by s against this his which gave t against against against against against begged himself which him against begged your against begged himself by over against against s who your begged these over against against against against s each himself against ahmed each his against begged himself by against against by by who your whom father which them against few each that your begged himself by against against against by who against against these that ah our begged himself against against against by by who take against begged himself this against begged himself him against against s each his against or this would against against against by by who your may were begged against each his ah against against against begged himself an against against these ah ll against against each his must ah t against begged himself against against against against s who spite which against against against against against against ah own against begged himself an which \n"
     ]
    }
   ],
   "source": [
    "ask_bot_story()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dafc77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
